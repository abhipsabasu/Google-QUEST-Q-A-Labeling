{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIVjtVUkExBZ",
        "colab_type": "text"
      },
      "source": [
        "**Kaggle Challenge: Google-QUEST-Q-A-Labeling**\n",
        "\n",
        "This challenge is mainly regression based, where each example data consists of a few question and answer features respectively, and 30 output variables, whose values have to be estimated. The following notebook consists of the central BERT-based model which has been used for this challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SG7owadpvk3",
        "colab_type": "code",
        "outputId": "e0a101db-5642-4bf5-a0d2-aff5e154713b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "dataset_path = 'gdrive/My Drive/Projects/quest/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9o7O-MHp5yB",
        "colab_type": "code",
        "outputId": "cc83dbb6-fc4a-4552-89f6-609d5fa44bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.1.0-rc2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.1.0-rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/44/4e8cc8c84cf235628ee919ba97ee029f2d080fa3573e26fe726973d004b4/tensorflow-2.1.0rc2-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.1.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.17.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.34.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.12.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 46.4MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0-rc2) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0-rc2) (45.1.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (0.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (2.21.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (4.0.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (3.0.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (3.1.0)\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.11.0 tensorboard-2.1.0 tensorflow-2.1.0rc2 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPnOIig1q4qZ",
        "colab_type": "code",
        "outputId": "276121cf-aebf-4fe1-e6a3-afc35f90807c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "!pip install sacremoses"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\r\u001b[K     |▍                               | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 4.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 6.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 6.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 7.1MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 7.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 122kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 133kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 143kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 153kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 174kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 184kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 194kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 204kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 215kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 225kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 235kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 245kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 256kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 266kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 276kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 286kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 296kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 307kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 317kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 327kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 337kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 348kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 358kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 368kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 378kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 389kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 399kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 409kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 419kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 430kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 440kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 450kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 460kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 471kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 481kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 491kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 501kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 512kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 522kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 532kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 542kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 552kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 563kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 573kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 583kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 593kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 604kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 614kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 624kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 634kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 645kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 655kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 665kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 675kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 686kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 696kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 706kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 716kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 727kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 737kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 747kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 757kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 768kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 778kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 788kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 798kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 808kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 819kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 829kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 839kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 849kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 860kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 870kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.28.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=74f7152dcb862764a421c212a419af3aaa432f9080a5a5af08d23f4b2a92b5e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR8erWrQrv3U",
        "colab_type": "code",
        "outputId": "20090748-b202-4189-b186-5524682a6d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\r\u001b[K     |▊                               | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 6.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 6.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 153kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 204kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 245kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 256kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 307kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 348kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 358kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 389kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 399kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 409kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 440kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 450kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\n",
            "Installing collected packages: tokenizers, sentencepiece, transformers\n",
            "Successfully installed sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWP4M8b_ryZI",
        "colab_type": "code",
        "outputId": "3de5b2ff-2a22-416d-8b2c-9353f5c18444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "# import bert_tokenization as tokenization\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
        "import os\n",
        "from scipy.stats import spearmanr\n",
        "from math import floor, ceil\n",
        "from transformers import *\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "np.set_printoptions(suppress=True)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwW7OAKZEb_i",
        "colab_type": "text"
      },
      "source": [
        "This cell specifies the **BERT tokenizer** to be used, and reads the data. Here, the uncased bert version is used with 12 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zA64IbXr2au",
        "colab_type": "code",
        "outputId": "525df0ce-d37b-44fa-902a-a42d77c3ed5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "PATH = dataset_path\n",
        "\n",
        "# BERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\n",
        "# tokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n",
        "\n",
        "BERT_PATH = dataset_path + 'bert-base-uncased-huggingface-transformer/'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt',do_lower_case = True)\n",
        "\n",
        "#tokenizer.add_tokens(['[Q-TITLE]'])\n",
        "#l = len(tokenizer)\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "\n",
        "df_train = pd.read_csv(PATH+'train.csv')\n",
        "df_test = pd.read_csv(PATH+'test.csv')\n",
        "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
        "print('train shape =', df_train.shape)\n",
        "print('test shape =', df_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train shape = (6079, 41)\n",
            "test shape = (476, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFdePahqIabE",
        "colab_type": "text"
      },
      "source": [
        "Retrieving the input features and output categories of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAMFljm6r5ux",
        "colab_type": "code",
        "outputId": "d488a7b7-1351-454b-f472-42d7a47dcea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "output_categories_qn = list(df_train.columns[11:32])\n",
        "output_categories_ans = list(df_train.columns[32:])\n",
        "input_categories = list(df_train.columns[[1,2,5]])\n",
        "print('\\noutput categories:\\n\\t', len(output_categories_qn))\n",
        "print('\\ninput categories:\\n\\t', len(output_categories_ans))\n",
        "output_categories = output_categories_qn+output_categories_ans"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "output categories:\n",
            "\t 21\n",
            "\n",
            "input categories:\n",
            "\t 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_yWylYiIo_V",
        "colab_type": "text"
      },
      "source": [
        "**Processing of Input Data**\n",
        "\n",
        "Each input example consists of the question title, question body, and the answer body. These input examples are then passed on to the BERT tokenizer, in two ways (one consists of the question title and body, the other consists of the question title and answer body), which then separates the data into two sets of ids, masks and segments, one for the question, the other for the answer.\n",
        "\n",
        "BERT accepts input vectors of length 512 only. To incorporate most information into the input vectors, the first 200 components and the last 312 components are taken of the ids and segments that are obtained as outputs of the BERT tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY3pMaC5r8iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
        "    \n",
        "    def return_id(str1, str2, truncation_strategy, length):\n",
        "\n",
        "        inputs = tokenizer.encode_plus(str1, str2,\n",
        "            add_special_tokens=True,\n",
        "            )\n",
        "        \n",
        "        input_ids =  inputs[\"input_ids\"]\n",
        "        input_segments = inputs[\"token_type_ids\"]\n",
        "        if len(input_ids) > length:\n",
        "          input_ids = input_ids[:200] + input_ids[-312:]\n",
        "          input_segments = input_segments[:200] + input_segments[-312:]\n",
        "\n",
        "        input_masks = [1] * len(input_ids)\n",
        "        padding_length = length - len(input_ids)\n",
        "        padding_id = tokenizer.pad_token_id\n",
        "        input_ids = input_ids + ([padding_id] * padding_length)\n",
        "        input_masks = input_masks + ([0] * padding_length)\n",
        "        input_segments = input_segments + ([0] * padding_length)\n",
        "        \n",
        "        return [input_ids, input_masks, input_segments]\n",
        "    \n",
        "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
        "        title+\" \"+question, None, 'longest_first', max_sequence_length)\n",
        "    \n",
        "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
        "        title + \" \" + answer, None, 'longest_first', max_sequence_length)\n",
        "    \n",
        "    return [input_ids_q, input_masks_q, input_segments_q,\n",
        "            input_ids_a, input_masks_a, input_segments_a]\n",
        "\n",
        "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
        "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
        "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
        "    for _, instance in tqdm(df[columns].iterrows()):\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "\n",
        "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
        "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        \n",
        "        input_ids_q.append(ids_q)\n",
        "        input_masks_q.append(masks_q)\n",
        "        input_segments_q.append(segments_q)\n",
        "\n",
        "        input_ids_a.append(ids_a)\n",
        "        input_masks_a.append(masks_a)\n",
        "        input_segments_a.append(segments_a)\n",
        "        \n",
        "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
        "            np.asarray(input_masks_q, dtype=np.int32), \n",
        "            np.asarray(input_segments_q, dtype=np.int32),\n",
        "            np.asarray(input_ids_a, dtype=np.int32), \n",
        "            np.asarray(input_masks_a, dtype=np.int32), \n",
        "            np.asarray(input_segments_a, dtype=np.int32)]\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOneHEnRVgMe",
        "colab_type": "code",
        "outputId": "44d67ee9-5d5a-47d7-f1d3-edfb85796acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT1r0XM1KPgB",
        "colab_type": "text"
      },
      "source": [
        "**BERT - LSTM model**\n",
        "\n",
        "This model is a concatenation of two branch models, one of the question and one for the answer. The basic construction of the two branches is the same. Both take as input their respective id, mask and segment, pass them onto the pretrained BERT model. Thereafter, the last four hidden layers of the BERT model are concatenated and passed on to a bi-LSTM of 512 cells. This layer is passed on to a pooling layer which is the final layer of the branch. After that, the two branches are concatenated, to which a drop out layer is added. The next layer is the output layer consisting of 30 cells for the corresponding output variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdoMk-Brr_bY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_spearmanr_ignore_nan(trues, preds):\n",
        "    rhos = []\n",
        "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
        "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
        "    return np.nanmean(rhos)\n",
        "\n",
        "def create_model_qn():\n",
        "    q_id_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    q_mask_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    q_atn_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    config = BertConfig() # print(config) to see settings\n",
        "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
        "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
        "    \n",
        "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
        "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
        "    bert_model = TFBertModel.from_pretrained(\n",
        "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
        "    #bert_model.resize_token_embeddings(30523)\n",
        "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
        "    #outputs = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[2]\n",
        "    \n",
        "    \n",
        "    #l_1, l_2, l_3, l_4 = outputs[-1], outputs[-2], outputs[-3], outputs[-4]\n",
        "\n",
        "    q_embedding_1 = bert_model(q_id_1, attention_mask=q_mask_1, token_type_ids=q_atn_1)[0]\n",
        "    \n",
        "    #q_embedding = tf.keras.layers.concatenate([l_1, l_2, l_3, l_4])\n",
        "    \n",
        "    q_1 = tf.keras.layers.GlobalAveragePooling1D()(q_embedding_1)\n",
        "        \n",
        "    x_1 = tf.keras.layers.Dropout(0.2)(q_1)\n",
        "    \n",
        "    x_1 = tf.keras.layers.Dense(21, activation='sigmoid')(x_1)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[q_id_1, q_mask_1, q_atn_1], outputs=x_1)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_model_ans():\n",
        "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    \n",
        "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    \n",
        "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    config = BertConfig() # print(config) to see settings\n",
        "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
        "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
        "    \n",
        "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
        "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
        "    bert_model = TFBertModel.from_pretrained(\n",
        "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
        "    #bert_model.resize_token_embeddings(30523)\n",
        "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
        "    outputs = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[2]\n",
        "    \n",
        "    l_1, l_2, l_3, l_4 = outputs[-1], outputs[-2], outputs[-3], outputs[-4]\n",
        "    \n",
        "    q_embedding = tf.keras.layers.concatenate([l_1, l_2, l_3, l_4])\n",
        "    q_embedding = Bidirectional(LSTM(512, return_sequences=True))(q_embedding)\n",
        "    #q_embedding = Bidirectional(LSTM(128, return_sequences=True))(q_embedding)\n",
        "    #q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
        "    \n",
        "    outputs_ans = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[2]\n",
        "    \n",
        "    a_1, a_2, a_3, a_4 = outputs_ans[-1], outputs_ans[-2], outputs_ans[-3], outputs_ans[-4]\n",
        "    \n",
        "    a_embedding = tf.keras.layers.concatenate([a_1, a_2, a_3, a_4])\n",
        "    a_embedding = Bidirectional(LSTM(512, return_sequences=True))(a_embedding)\n",
        "    #a_embedding = Bidirectional(LSTM(128, return_sequences=True))(a_embedding)\n",
        "    #q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
        "    #a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
        "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
        "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
        "    \n",
        "    x = tf.keras.layers.Concatenate()([q, a])\n",
        "    \n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn,a_id, a_mask, a_atn], outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOkMNdm4sBXR",
        "colab_type": "code",
        "outputId": "c9c7a6b8-f483-4e4f-dc1d-c40c1b9e0b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#outputs_qn = compute_output_arrays(df_train, output_categories_qn)\n",
        "#outputs_ans = compute_output_arrays(df_train, output_categories_ans)\n",
        "outputs = compute_output_arrays(df_train, output_categories)\n",
        "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6079it [00:47, 132.33it/s]\n",
            "476it [00:03, 124.65it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nts8MCUxNCAl",
        "colab_type": "text"
      },
      "source": [
        "The training is performed at this stage, with 10 fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7VCq1DnsDZS",
        "colab_type": "code",
        "outputId": "32e8eafe-d00e-4cfb-ad2f-b35daf9a6051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)\n",
        "\n",
        "valid_preds = []\n",
        "test_preds = []\n",
        "K.clear_session()\n",
        "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
        "    \n",
        "    # will actually only do 2 folds (out of 5) to manage < 2h\n",
        "    if fold in range(10):\n",
        "\n",
        "        #train_inputs_qn = [inputs[i][train_idx] for i in range(3)]\n",
        "        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
        "        train_outputs = outputs[train_idx]\n",
        "        #train_outputs_qn = outputs_qn[train_idx]\n",
        "        #train_outputs_ans = outputs_ans[train_idx]\n",
        "        \n",
        "        #valid_inputs_qn = [inputs[i][valid_idx] for i in range(3)]\n",
        "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
        "        valid_outputs = outputs[valid_idx]\n",
        "        #valid_outputs_qn = outputs_qn[valid_idx]\n",
        "        #valid_outputs_ans = outputs_ans[valid_idx]\n",
        "        \n",
        "       \n",
        "        \n",
        "        #model = create_model_qn()\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "        #model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        #model.fit(train_inputs_qn, train_outputs_qn, epochs=1, batch_size=6)\n",
        "        #model.save_weights(dataset_path + 'bert-ques'+str(fold)+'.h5')'''\n",
        "        K.clear_session()\n",
        "        model1 = create_model_ans()\n",
        "        model1.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        model1.fit(train_inputs, train_outputs, epochs=3, batch_size=6)\n",
        "        model1.save_weights(dataset_path + 'bert-ans'+str(fold)+'.h5')\n",
        "        \n",
        "        valid_preds.append(model1.predict(valid_inputs))\n",
        "        test_preds.append(model1.predict(test_inputs))\n",
        "        #valid_outputs = np.column_stack((valid_outputs_qn,valid_outputs_ans))\n",
        "        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
        "        print('validation score = ', rho_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6d20d324f791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalid_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GroupKFold' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UT5TbdMNikb",
        "colab_type": "text"
      },
      "source": [
        "There are a number of variants of the model mentioned above. \n",
        "\n",
        "1.   Use bi-GRU of the same number of cells instead of bi_LSTM\n",
        "2.   Adjust the number of LSTM/GRU cells\n",
        "3.   Instead of concatenating the last 4 hidden layers of the BERT model and passing it to bi-LSTM or bi-GRU, use the original output of the BERT model\n",
        "4.   Instead of creating two branches for questions and answers in the same model, create two separate models. One model will take into account the question based features and predict only the question based output variables, the other model will take in the question and answer based features, and predict the answer based output variables. The structure of the model can be any of the above 3 architectures. However, this has not been tried, as it usually consumes a lot of resource.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJNPIHdbtB6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}