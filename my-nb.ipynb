{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SG7owadpvk3",
        "colab_type": "code",
        "outputId": "e301f76b-61bf-4294-c01a-c00efe4a8f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "dataset_path = 'gdrive/My Drive/Projects/quest/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9o7O-MHp5yB",
        "colab_type": "code",
        "outputId": "6c66c074-7936-4658-8828-7297de9f9fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "!pip install tensorflow==2.1.0-rc2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.1.0-rc2 in /usr/local/lib/python3.6/dist-packages (2.1.0rc2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (2.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.34.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (3.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.17.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (2.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc2) (0.1.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0-rc2) (45.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (0.16.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (1.11.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0-rc2) (2.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (1.24.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc2) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPnOIig1q4qZ",
        "colab_type": "code",
        "outputId": "b67a301f-c18f-4eb7-d963-683a9bbfec99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "!pip install sacremoses"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.28.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=9c6afe33f5a6289074ac22e69d6c1af8ed4b34ca939c0a18d65488247fe8ac35\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR8erWrQrv3U",
        "colab_type": "code",
        "outputId": "9ce6b1c1-8be5-421e-ab84-ef97e606eb58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\r\u001b[K     |▊                               | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.9)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.6.1)\n",
            "Installing collected packages: sentencepiece, tokenizers, transformers\n",
            "Successfully installed sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWP4M8b_ryZI",
        "colab_type": "code",
        "outputId": "59c12a58-f42f-4283-dba1-261442783ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "# import bert_tokenization as tokenization\n",
        "import tensorflow.keras.backend as K\n",
        "import os\n",
        "from scipy.stats import spearmanr\n",
        "from math import floor, ceil\n",
        "from transformers import *\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "np.set_printoptions(suppress=True)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zA64IbXr2au",
        "colab_type": "code",
        "outputId": "7c016d31-53ff-4864-93e4-f064ad31a77e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "PATH = dataset_path\n",
        "\n",
        "# BERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\n",
        "# tokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n",
        "\n",
        "BERT_PATH = dataset_path + 'bert-base-uncased-huggingface-transformer/'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
        "\n",
        "#tokenizer.add_tokens(['[Q-TITLE]'])\n",
        "#l = len(tokenizer)\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "\n",
        "df_train = pd.read_csv(PATH+'train.csv')\n",
        "df_test = pd.read_csv(PATH+'test.csv')\n",
        "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
        "print('train shape =', df_train.shape)\n",
        "print('test shape =', df_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train shape = (6079, 41)\n",
            "test shape = (476, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAMFljm6r5ux",
        "colab_type": "code",
        "outputId": "133254d9-5538-4f30-efdd-462fd7f54edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "output_categories_qn = list(df_train.columns[11:32])\n",
        "output_categories_ans = list(df_train.columns[32:])\n",
        "input_categories = list(df_train.columns[[1,2,5]])\n",
        "print('\\noutput categories:\\n\\t', len(output_categories_qn))\n",
        "print('\\ninput categories:\\n\\t', len(output_categories_ans))\n",
        "output_categories = output_categories_qn+output_categories_ans"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "output categories:\n",
            "\t 21\n",
            "\n",
            "input categories:\n",
            "\t 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY3pMaC5r8iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
        "    \n",
        "    def return_id(str1, str2, truncation_strategy, length):\n",
        "\n",
        "        inputs = tokenizer.encode_plus(str1, str2,\n",
        "            add_special_tokens=True,\n",
        "            )\n",
        "        \n",
        "        input_ids =  inputs[\"input_ids\"]\n",
        "        input_segments = inputs[\"token_type_ids\"]\n",
        "        if len(input_ids) > length:\n",
        "          input_ids = input_ids[:200] + input_ids[-312:]\n",
        "          input_segments = input_segments[:200] + input_segments[-312:]\n",
        "\n",
        "        input_masks = [1] * len(input_ids)\n",
        "        padding_length = length - len(input_ids)\n",
        "        padding_id = tokenizer.pad_token_id\n",
        "        input_ids = input_ids + ([padding_id] * padding_length)\n",
        "        input_masks = input_masks + ([0] * padding_length)\n",
        "        input_segments = input_segments + ([0] * padding_length)\n",
        "        \n",
        "        return [input_ids, input_masks, input_segments]\n",
        "    \n",
        "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
        "        title+\" \"+question, None, 'longest_first', max_sequence_length)\n",
        "    \n",
        "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
        "        title + \" \" + answer, None, 'longest_first', max_sequence_length)\n",
        "    \n",
        "    return [input_ids_q, input_masks_q, input_segments_q,\n",
        "            input_ids_a, input_masks_a, input_segments_a]\n",
        "\n",
        "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
        "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
        "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
        "    for _, instance in tqdm(df[columns].iterrows()):\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "\n",
        "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
        "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        \n",
        "        input_ids_q.append(ids_q)\n",
        "        input_masks_q.append(masks_q)\n",
        "        input_segments_q.append(segments_q)\n",
        "\n",
        "        input_ids_a.append(ids_a)\n",
        "        input_masks_a.append(masks_a)\n",
        "        input_segments_a.append(segments_a)\n",
        "        \n",
        "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
        "            np.asarray(input_masks_q, dtype=np.int32), \n",
        "            np.asarray(input_segments_q, dtype=np.int32),\n",
        "            np.asarray(input_ids_a, dtype=np.int32), \n",
        "            np.asarray(input_masks_a, dtype=np.int32), \n",
        "            np.asarray(input_segments_a, dtype=np.int32)]\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdoMk-Brr_bY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_spearmanr_ignore_nan(trues, preds):\n",
        "    rhos = []\n",
        "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
        "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
        "    return np.nanmean(rhos)\n",
        "\n",
        "def create_model_qn():\n",
        "    q_id_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    q_mask_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    q_atn_1 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    config = BertConfig() # print(config) to see settings\n",
        "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
        "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
        "    \n",
        "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
        "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
        "    bert_model = TFBertModel.from_pretrained(\n",
        "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
        "    #bert_model.resize_token_embeddings(30523)\n",
        "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
        "    #outputs = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[2]\n",
        "    \n",
        "    \n",
        "    #l_1, l_2, l_3, l_4 = outputs[-1], outputs[-2], outputs[-3], outputs[-4]\n",
        "\n",
        "    q_embedding_1 = bert_model(q_id_1, attention_mask=q_mask_1, token_type_ids=q_atn_1)[0]\n",
        "    \n",
        "    #q_embedding = tf.keras.layers.concatenate([l_1, l_2, l_3, l_4])\n",
        "    \n",
        "    q_1 = tf.keras.layers.GlobalAveragePooling1D()(q_embedding_1)\n",
        "        \n",
        "    x_1 = tf.keras.layers.Dropout(0.2)(q_1)\n",
        "    \n",
        "    x_1 = tf.keras.layers.Dense(21, activation='sigmoid')(x_1)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[q_id_1, q_mask_1, q_atn_1], outputs=x_1)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_model_ans():\n",
        "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    \n",
        "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    \n",
        "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    config = BertConfig() # print(config) to see settings\n",
        "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
        "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
        "    \n",
        "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
        "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
        "    bert_model = TFBertModel.from_pretrained(\n",
        "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
        "    #bert_model.resize_token_embeddings(30523)\n",
        "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
        "    #outputs = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[2]\n",
        "    \n",
        "    #l_1, l_2, l_3, l_4 = outputs[-1], outputs[-2], outputs[-3], outputs[-4]\n",
        "    \n",
        "    #q_embedding = tf.keras.layers.concatenate([l_1, l_2, l_3, l_4])\n",
        "\n",
        "    #q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
        "    \n",
        "    #outputs_ans = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[2]\n",
        "    \n",
        "    #a_1, a_2, a_3, a_4 = outputs_ans[-1], outputs_ans[-2], outputs_ans[-3], outputs_ans[-4]\n",
        "    \n",
        "    #ans_embedding = tf.keras.layers.concatenate([a_1, a_2, a_3, a_4])\n",
        "\n",
        "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
        "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
        "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
        "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
        "    \n",
        "    x = tf.keras.layers.Concatenate()([q, a])\n",
        "    \n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn,a_id, a_mask, a_atn], outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOkMNdm4sBXR",
        "colab_type": "code",
        "outputId": "c7cb98d0-a579-4ab4-c296-b94018e04db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#outputs_qn = compute_output_arrays(df_train, output_categories_qn)\n",
        "#outputs_ans = compute_output_arrays(df_train, output_categories_ans)\n",
        "outputs = compute_output_arrays(df_train, output_categories)\n",
        "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6079it [00:33, 178.97it/s]\n",
            "476it [00:02, 175.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7VCq1DnsDZS",
        "colab_type": "code",
        "outputId": "03d11da6-a75e-466a-f426-afc1ca09f7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "gkf = GroupKFold(n_splits=7).split(X=df_train.question_body, groups=df_train.question_body)\n",
        "\n",
        "valid_preds = []\n",
        "test_preds = []\n",
        "K.clear_session()\n",
        "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
        "    \n",
        "    # will actually only do 2 folds (out of 5) to manage < 2h\n",
        "    if fold in range(7):\n",
        "\n",
        "        #train_inputs_qn = [inputs[i][train_idx] for i in range(3)]\n",
        "        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
        "        train_outputs = outputs[train_idx]\n",
        "        #train_outputs_qn = outputs_qn[train_idx]\n",
        "        #train_outputs_ans = outputs_ans[train_idx]\n",
        "        \n",
        "        #valid_inputs_qn = [inputs[i][valid_idx] for i in range(3)]\n",
        "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
        "        valid_outputs = outputs[valid_idx]\n",
        "        #valid_outputs_qn = outputs_qn[valid_idx]\n",
        "        #valid_outputs_ans = outputs_ans[valid_idx]\n",
        "        \n",
        "       \n",
        "        \n",
        "        #model = create_model_qn()\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "        #model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        #model.fit(train_inputs_qn, train_outputs_qn, epochs=1, batch_size=6)\n",
        "        #model.save_weights(dataset_path + 'bert-ques'+str(fold)+'.h5')'''\n",
        "        K.clear_session()\n",
        "        model1 = create_model_ans()\n",
        "        model1.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        model1.fit(train_inputs, train_outputs, epochs=4, batch_size=6)\n",
        "        model1.save_weights(dataset_path + 'bert-ans'+str(fold)+'.h5')\n",
        "        \n",
        "        valid_preds.append(model1.predict(valid_inputs))\n",
        "        test_preds.append(model1.predict(test_inputs))\n",
        "        #valid_outputs = np.column_stack((valid_outputs_qn,valid_outputs_ans))\n",
        "        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
        "        print('validation score = ', rho_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5210 samples\n",
            "Epoch 1/4\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "5210/5210 [==============================] - 1367s 262ms/sample - loss: 0.3935\n",
            "Epoch 2/4\n",
            "5210/5210 [==============================] - 1352s 259ms/sample - loss: 0.3653\n",
            "Epoch 3/4\n",
            "5210/5210 [==============================] - 1350s 259ms/sample - loss: 0.3510\n",
            "Epoch 4/4\n",
            "5210/5210 [==============================] - 1350s 259ms/sample - loss: 0.3369\n",
            "validation score =  0.38394643304039816\n",
            "Train on 5210 samples\n",
            "Epoch 1/4\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "5210/5210 [==============================] - 1375s 264ms/sample - loss: 0.3931\n",
            "Epoch 2/4\n",
            "5210/5210 [==============================] - 1351s 259ms/sample - loss: 0.3652\n",
            "Epoch 3/4\n",
            "5210/5210 [==============================] - 1351s 259ms/sample - loss: 0.3513\n",
            "Epoch 4/4\n",
            "5210/5210 [==============================] - 1350s 259ms/sample - loss: 0.3370\n",
            "validation score =  0.39603775819158515\n",
            "Train on 5210 samples\n",
            "Epoch 1/4\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "5210/5210 [==============================] - 1375s 264ms/sample - loss: 0.3926\n",
            "Epoch 2/4\n",
            "5210/5210 [==============================] - 1351s 259ms/sample - loss: 0.3640\n",
            "Epoch 3/4\n",
            "5210/5210 [==============================] - 1352s 259ms/sample - loss: 0.3496\n",
            "Epoch 4/4\n",
            "5210/5210 [==============================] - 1353s 260ms/sample - loss: 0.3363\n",
            "validation score =  0.3898665451181586\n",
            "Train on 5211 samples\n",
            "Epoch 1/4\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "5211/5211 [==============================] - 1376s 264ms/sample - loss: 0.3952\n",
            "Epoch 2/4\n",
            "5211/5211 [==============================] - 1352s 259ms/sample - loss: 0.3654\n",
            "Epoch 3/4\n",
            "2514/5211 [=============>................] - ETA: 11:39 - loss: 0.3521"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4UVrTtgsXMf",
        "colab_type": "code",
        "outputId": "81713698-ffa8-4f58-8216-cfdf255db91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb  2 12:34:23 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJNPIHdbtB6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}